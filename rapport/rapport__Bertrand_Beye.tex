%! Author = vadim
%! Date = 1/7/23

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage[sorting=none,block=ragged,style=numeric-comp]{biblatex}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{dirtytalk}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\usepackage{xcolor}

% \addbibresource{ref.bib}
\graphicspath{{images/}}
\hypersetup{
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue}
}
\urlstyle{same}

\title{Résistance aux antibiotiques}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Prédiction par apprentissage statistique}
\author{Vadim BERTRAND, Cheikh-Darou BEYE}
\date{9 janvier 2023}

\makeatletter
\renewcommand*{\fps@figure}{H}
\makeatother

% Document
\begin{document}
\maketitle

\renewcommand*\contentsname{Sommaire}{
  \hypersetup{linkcolor=}
  \setcounter{tocdepth}{3}
  \tableofcontents
}
\newpage

\hypertarget{antibioresistance}{%
\section{L'antibiorésistance}\label{antibioresistance}}

  Les antibiotiques sont développés pour contrer les infections dues aux bactéries.
  Certaines bactéries peuvent acquérir une résistance à des antibiotiques, via l'obtention de nouveaux gènes ou par la mutation de gènes existants.
  L'antibiorésistance représente un grand risque pour la santé publique, il est donc important de la limiter.
  Cela passe notamment par une meilleure compréhension des mécanismes de résistances comme l'identification de gènes résistants ou des bactéries résistantes.
  Cette résistance de certaines bactéries à des antibiotiques peut être traitée comme une tâche de classification en apprentissage statistique.

  Dans cette étude nous aurons à notre disposition un jeu de données constitué de 3 matrices de régresseurs et une matrice réponse pour 414 bactéries :
  \begin{itemize}
    \item \textit{X\_gpa}, codant la présence ou l'absence de 16005 gènes,
    \item \textit{X\_nsps}, codant la présence ou l'absence de 72236 mutations génétiques,
    \item \textit{X\_genexp}, représentant l'expression génétique de 6026 gènes ;
    \item \textit{Y}, codant la résistance ou la sensibilité à 5 antibiotiques : la Ceftazidime, la Ciprofloxacine, la Colistine, le Méropenème et la Tobramycine.
  \end{itemize}
  Notre objectif est de prédire la résistance des bactéries aux antibiotiques à partir des régresseurs et d'identifier quelles matrices de régresseurs sont les plus intéressantes pour cette tâche, selon l'antibiotique considéré.

  Dans un premier temps, nous procéderons à une courte exploration des données.
  Puis, nous détaillerons notre démarche : pré-traitements utilisés sur les données, proposition d'approches de réduction de dimension, classifieurs considérés et mise en œuvre via la librairie \textit{scikit-learn}.
  Enfin, nous présenterons les résultats obtenus et nous proposerons quelques pistes d'amélioration.

\hypertarget{exploration-donnees}{%
\section{Exploration des données}\label{exploration-donnees}}

  Avant de nous lancer dans la prédiction de la résistance aux antibiotiques, nous avons souhaité nous pencher sur les données que nous manipulons.

  Naturellement nous avons commencé par observer les types de données que nous manipulons et l'éventuelle présence de données manquantes.
  Sur les 4 matrices à notre disposition, 3 contiennent des données binaires (\textit{Y}, \textit{X\_gpa}, \textit{X\_nsps}) tandis que \textit{X\_genexp} contient des données quantitatives.

  Comme le montre la table~\ref{tab:data}, les matrices de régresseurs ne contiennent pas de données manquantes, mais certaines informations de résistance aux antibiotiques sont manquantes, notamment pour la Ceftazidim avec 20\% de données absentes.
  Etant donné que la taille du jeu de données est réduite, que nous procèderons par la suite à une validation croisée et que nous ne disposerons donc pas d'un jeu de test, nous avons choisi de ne pas imputer les données manquantes afin d'éviter de fausser la généralisation des résultats.
  Par conséquent, les bactéries dont la résistance à un antibiotique est manquante ne seront pas utilisées lors de l'évaluation des classifieurs sur l'antibiotique correspondant.

  Nous pouvons également observer sur la table~\ref{tab:data} que les variables réponses ne sont pas toujours équilibrées : 2 fois plus de bactéries résistantes à la Méropenem, et à l'inverse 2 à 3 fois plus de bactéries susceptibles à la Tobramycin et la Colistin.
  De même, les gènes ou les mutations sont bien plus souvent absentes que présentes.

  \input{tables/data.tex}

  Pour aller un peu plus loin, nous avons représenté nos données regroupées par clustering hiérarchique avec des cartes de chaleur afin de faire apparaître des structures.
  La figure~\ref{fig:gpa_cm} correspondant à la carte de chaleur ainsi obtenue pour la matrice \textit{X\_gpa} permet par exemple de supposer que cette matrice porte de l'information intéressante pour prédire la résistance à la Tobramycin et la Ciprofloxacine, mais probablement moins pour la Colistin.
  Pour celle-ci, nous avons observé par le même biais que la matrice \textit{X\_genexp} sera sûrement indispensable.

  \begin{figure}[H]
    \centering
    \includegraphics[height=10cm,width=\textwidth,keepaspectratio]{gpa_cm}
    \caption{Carte de chaleur du clustering hiérarchique sur les lignes et les colonnes de la matrice \textit{X\_gpa}.
    Les couleurs à gauche des lignes permettent de déterminer si la bactérie est sensible (bleu) ou résistante (rouge) à l'antibiotique, le gris correspond aux données manquantes.}
    \label{fig:gpa_cm}
  \end{figure}

\hypertarget{approche-prediction}{%
\section{Approche pour la prédiction}\label{approche-prediction}}

\hypertarget{pre-traitements}{%
\subsection{Pré-traitements}\label{pre-traitements}}

  Comme expliqué dans la section~\hyperref[exploration-donnees]{\S2}, nous avons fait le choix de supprimer les données manquantes.
  Cette suppression est faite de manière \say{intelligente} en ce sens où les bactéries dont la résistance est absente sont éliminées uniquement pour les antibiotiques concernés et demeurent disponible pour les autres antibiotiques.

  Nous nous sommes ensuite contentés de centrer/réduire les expressions génétiques de la matrice \textit{X\_genexp} grâce au transformateur \textbf{StandardScaler} de \textit{scikit-learn}.

  S'est ensuite posée la question de la mutualisation des informations contenues dans les 3 matrices de classifieurs.
  En traçant les cartes de chaleur de ces matrices, nous avons aperçu que toutes ne sont pas nécessairement pertinentes pour tous les antibiotiques.
  C'est pourquoi nous avons fait le choix de ne pas systématiquement agréger les matrices, mais de considérer les 7 arrangements possibles : 1 seule matrice (3), 2 matrices (3) et les 3 matrices (1).

\hypertarget{reduction-dimension}{%
\subsection{Réduction de la dimensionnalité}\label{reduction-dimension}}

  Nous avons vu que nos matrices de régresseurs contiennent beaucoup de covariables, jusqu'à 94267 lorsque nous les concaténons entres-elles, relativement aux nombres d'observations dont nous disposons.
  Il est donc impératif d'envisager de réduire cette dimensionnalité.
  Cela peut se faire en amont de la tâche de classification, ou alors de manière intégrée en incluant une pénalisation sur les poids du modèle associés aux regresseurs.
  Nous avons considéré les deux approches et nous détaillerons dans cette partie les 3 méthodes que nous avons employées en amont.

\hypertarget{acp-noyau}{%
\subsubsection{ACP (à Noyau)}\label{acp-noyau}}

  L'Analyse en Composantes Principales (ACP) est une approche bien connue permettant de représenter les observations dans un sous-espace vectoriel dont les composantes sont décorrélées.
  Les composantes sont ordonnées en ordre décroissant de variance expliquée, de sorte qu'il est simple d'utiliser le nombre de composantes conservées pour représenter les données comme un hyper-paramètre permettant de plus ou moins réduire la dimensionnalité tout en maximisant le pourcentage de variance expliquée pour un nombre de composantes fixé.

  \begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{pca_var}
      \caption{Variance cumulée exprimée en pourcentage.}
      \label{fig:pca_var}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{pca}
      \caption{Nuage de points selon les 2 premières dimensions.}
      \label{fig:pca_proj}
    \end{subfigure}
    \hfill
    \caption{Représentations de l'ACP avec un noyau linéaire.}
    \label{fig:pca}
  \end{figure}

  Nous avons appliqué l'ACP à notre jeu de données, en considérant tous les arrangements possibles d'utilisation des matrices de descripteurs et comme nous pouvons le voir sur la figure~\ref{fig:pca}, les résultats sont mitigés.
  D'un côté, la figure~\ref{fig:pca_var} ne montre pas de point d'inflexion franc dans la courbe de variance cumulée, ce qui suggère qu'il n'existe pas de point au delà duquel les composantes sont peu importantes.
  En revanche, nous pouvons voir sur la figure~\ref{fig:pca_proj} que la projection dans les deux premières dimensions de l'ACP (en ayant agrégé l'ensemble des matrices de régresseurs) de nos données permet de partiellement séparer les bactéries sensibles et résistantes, ce qui est encourageant en vue de l'étape de classification.

  Par ailleurs, grâce au \textit{kernel-trick} il est possible de mettre en place une variante de l'ACP permettant une réduction de dimensions non linéaire en utilisant divers noyaux.
  Via \textit{scikit-learn}, il suffit alors d'utiliser la classe \textbf{KernelPCA}.

\hypertarget{stability-selection}{%
\subsubsection{Stability selection}\label{stability-selection}}

  La \textit{stability selection} est une approche permettant de corriger l'instabilité de la sélection de variables de l'estimateur Lasso en présence de covariables corrélées.
  Pour ce faire, des estimateurs pénalisés avec la norme $L1$ sont ajustés sur de multiples ré-échantillonnages du jeu de données.
  Chaque estimateur produit un chemin de régularisation le long duquel les poids associés aux covariables les moins importantes pour la tâche de régression ou de classification sont progressivement annulés quand la pénalisation augmente.
  À partir de ces chemins de régularisation, l'estimateur de \textit{stability selection} construit un chemin de stabilité représentant la fréquence de sélection des covariables parmi l'ensemble des estimateurs Lasso.
  Au moyen d'un seuil de stabilité, il est donc possible de sélectionner les variables alors considérées comme \textit{stables}.
  Cette procédure est généralement utilisée comme un estimateur plutôt que comme une méthode de réduction de dimensions, mais nous avons souhaité la tester ainsi (étant donné que nous avons intégré le Lasso aux classifieurs considérés par la suite, l'approche originelle est cependant également employée).

  La \textit{stability selection} n'est pas proposée par \textit{scikit-learn} (uniquement le \textit{bootsrap Lasso}), nous avons donc implémenté notre propre estimateur héritant des classes de bases \textbf{TransformerMixin} et \textbf{BaseEstimator} afin de pouvoir l'utiliser ensuite dans un \textbf{Pipeline}.
  La figure~\ref{fig:stab_path} illustre le chemin de régularité obtenu en utilisant l'ensemble des matrices de régresseurs pour construire la matrice de covariables et avec un seuil de régularité de $0.6$.
  Un tel seuil conduit à conserver environ $1250$ covariables sur plus de $96000$ au départ.

  \begin{figure}[H]
    \centering
    \includegraphics[height=6cm,width=\textwidth,keepaspectratio]{stab_sel}
    \caption{Illustration du chemin de stabilité obtenu par \textit{stability selection}.}
    \label{fig:stab_path}
  \end{figure}

  L'un des inconvénients principal de la \textit{stability selection} est le temps d'ajustement très important en raison du nombre de ré-échantillonnages à effectuer (100 dans notre cas) et de la longueur du chemin de régularisation (fixé à 25).

\hypertarget{tests-multiples}{%
\subsubsection{Tests multiples}\label{tests-multiples}}

  Une autre manière de réaliser la réduction de dimensionnalité pourrait être de considérer que de nombreuses variables sont inutiles pour la prédiction, car leurs distributions entre les bactéries susceptibles ou résistantes est la même.

  Nous pouvons ainsi procéder à une nouvelle sélection de variables s'appuyant cette fois-ci sur deux tests statistiques : le Z-test pour les matrices \textit{X\_gpa} et \textit{X\_snps} et le T-test pour la matrice \textit{X\_genexp}.
  Nous avons utilisé la librairie \textit{statsmodels} qui propose l'implémentation de ces tests.
  En raison du grand nombre de tests réalisés de manière indépendante, il convient d'ajuster les $pvaleurs$.
  Nous avons choisi d'utiliser la procédure de Benjamini-Hochberg permettant de contrôler le FDR (\textit{False Discovery Rate}), plutôt que des procédures contrôlant le FWER (\textit{Family-Wise Error Rate}) afin de découvrir plus de variables statistiquement différentes selon les deux modalités de résistance.
  Nous nous sommes tourné pour cela vers la librairie \textit{MultiPy} qui permet de choisir parmi de nombreuses méthodes d'ajustement.
  L'ensemble de la procédure a été encapsulée dans une classe permettant de l'utiliser comme un transformateur dans un \textbf{Pipeline}.

  Comme le montre la table~\ref{tab:mul_test}, cette approche permet de réduire très fortement le nombre de covariables utilisées par le classifieur, et le niveau $\alpha$ peut-être employé comme un hyper-paramètre permettant de jouer sur le nombre de variables rejetées.

  \input{tables/mul_test.tex}

  Le désavantage que nous voyons à cette approche par tests multiples est que les variables sélectionnées sont potentiellement corrélées.

\hypertarget{classifieurs-consideres}{%
\subsection{Classifieurs considérés}\label{classifieurs-consideres}}

  Nous avons considéré 3 grands types de classifieurs différents : régression logistique, machines à support vectoriel (SVM) et forêts aléatoires.
  Nous avons également utilisé les variantes AdaBoost et Gradient Boosting des forêts aléatoires, ainsi que l'entrainement par descente de gradient pour la régression logistique et la SVM linéaire.

  Dans l'univers \textit{scikit-learn} cela revient à utiliser les classes \textbf{LogisticRegression}, \textbf{SVC}, \textbf{RandomForestClassifier}, \textbf{AdaBoostClassifier}, \textbf{GradientBoostingClassifier} et \textbf{SGDClassifier}.
  Nous verrons dans la section suivante quels hyper-paramètres nous avons fait varier pour ces différents estimateurs, mais pour l'ensemble d'entre eux nous avons spécifié \textit{class\_weight="balanced"} afin de tenir compte du déséquilibre entre les nombres de bactéries sensibles et résistantes.

\hypertarget{mise-oeuvre}{%
\subsection{Mise en œuvre}\label{mise-oeuvre}}

  Comme évoqué précédemment, nous nous sommes reposés sur la librairie \textit{scikit-learn} pour la mise en œuvre de notre étude de prédiction.

  \begin{figure}[H]
    \centering
    \includegraphics[height=6cm,width=\textwidth,keepaspectratio]{pipe}
    \caption{Représentation du \textbf{Pipeline} construit.}
    \label{fig:pipe}
  \end{figure}

  La figure~\ref{fig:pipe} illustre le \textbf{Pipeline} que nous avons utilisé.
  Le premier niveau, \textit{trans\_ind}, applique les pré-traitements de manière distincte pour les différentes matrices de régresseurs.
  Le niveau \textit{sel\_ind} permet de générer les différents arrangements possibles pour l'utilisation des matrices de régresseurs.
  Le troisième niveau correspond à celui de la réduction de dimension.
  Et enfin, le dernier estimateur est le classifieur.

  Ce \textbf{Pipeline} nous permet de \say{facilement} faire varier les différents estimateurs et leurs hyper-paramètres en utilisant la classe \textbf{GridSearchCV} pour trouver la combinaison la plus performante.
  \say{Facilement} car construire une grille d'hyper-paramètres avec notre structure n'est pas si évident que cela, et car le nombre de combinaisons d'hyper-paramètres augmente très vite.
  Nous avons indiqué à l'object \textbf{GridSearchCV} que la métrique à utiliser pour mesurer la performance est la \textit{balanced accuracy} afin de tenir compte du déséquilibre dans les classes à prédire.

  \input{tables/grid_params.tex}

  La table~\ref{tab:grid_params} donne à voir l'ensemble des hyper-paramètres considérés dans la grille de recherche.
  Au total, cela représente $7200$ combinaisons d'estimateurs et d'hyper-paramètres par antibiotique.
  Afin de limiter le temps de recherche des hyper-paramètres, en plus du parallélisme, nous avons utilisé la librairie \textit{scikit-learn-intelex} qui propose des implémentations plus rapides de certains estimateurs pour les processeurs Intel ;
  et nous nous sommes également servi de l'argument \textit{memory} de \textbf{GridSearchCV} qui permet de ne pas ré-ajuster les estimateurs lorsque leurs paramètres d'entrée ne changent pas.
  Malgré cela, la recherche des hyper-paramètres a nécessité plusieurs heures.

\hypertarget{resultats}{%
\section{Résultats}\label{resultats}}

  Dans un premier temps nous nous sommes intéressés à la moyenne et à la déviation standard des scores obtenus par validation croisée pendant la procédure de recherche des hyper-paramètres.
  Nous avons fait figurer sur la table~\ref{tab:res_cv} les meilleurs résultats selon ces critères pour chacun des antibiotiques.
  La table permet également de voir quelles matrices de régresseurs ont alors été utilisées, ainsi que les estimateurs de réduction de dimension et de classification retenus.

  \input{tables/res_cv.tex}



\hypertarget{perspectives}{%
\section{Perspectives}\label{perspectives}}

\end{document}